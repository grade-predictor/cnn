{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"capstone_resnet50.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":23,"metadata":{"id":"JyTGlLXfNLii","executionInfo":{"status":"ok","timestamp":1651729581171,"user_tz":-540,"elapsed":4,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"outputs":[],"source":["# torchvision 관련 라이브러리 import\n","\n","from torchvision import utils\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","source":[""],"metadata":{"id":"5uikNndWW_9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["0. 데이터셋 로드"],"metadata":{"id":"0GitHpa7XBmn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8x8E-xwbW_Qt","outputId":"8109c2d3-47df-4fc9-a790-9407546f9504","executionInfo":{"status":"ok","timestamp":1651729591192,"user_tz":-540,"elapsed":4828,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"YmBD1wKovqoI"}},{"cell_type":"code","source":["!pip install jsonlines\n","!pip install openreview-py\n","!pip install pdf2image"],"metadata":{"id":"u3gRUgpEW9cq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651729607057,"user_tz":-540,"elapsed":13411,"user":{"displayName":"정지우","userId":"03273628197078318973"}},"outputId":"4e0c18a5-97a2-4b70-ffaa-88d6757d91ed"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jsonlines in /usr/local/lib/python3.7/dist-packages (3.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (4.2.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n","Requirement already satisfied: openreview-py in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from openreview-py) (2.23.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from openreview-py) (0.16.0)\n","Requirement already satisfied: tld>=0.12 in /usr/local/lib/python3.7/dist-packages (from openreview-py) (0.12.6)\n","Requirement already satisfied: pylatexenc in /usr/local/lib/python3.7/dist-packages (from openreview-py) (2.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openreview-py) (4.64.0)\n","Requirement already satisfied: setuptools==49.6.0 in /usr/local/lib/python3.7/dist-packages (from openreview-py) (49.6.0)\n","Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from openreview-py) (3.14.1)\n","Requirement already satisfied: pyjwt in /usr/local/lib/python3.7/dist-packages (from openreview-py) (2.3.0)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.7/dist-packages (from openreview-py) (1.2.13)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->openreview-py) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->openreview-py) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->openreview-py) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->openreview-py) (1.24.3)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from Deprecated->openreview-py) (1.14.0)\n","Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (1.16.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n"]}]},{"cell_type":"code","source":["#@title\n","\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torchvision.models as models\n","import tensorflow as tf\n","import keras\n"],"metadata":{"id":"0wgRUMyyW9je","executionInfo":{"status":"ok","timestamp":1651729609832,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["#@title\n","from torch.utils.data import DataLoader\n","\n","from torchvision import transforms\n","import openreview\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import Dataset\n","import glob\n","from PIL import Image\n","import jsonlines\n","import os\n","from tqdm import tqdm"],"metadata":{"id":"XW5zuXp1W9nc","executionInfo":{"status":"ok","timestamp":1651729611389,"user_tz":-540,"elapsed":4,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["#@title\n","\n","class PaperDataSet(Dataset):\n","    def __init__(self, overall_image_path, transform=None):\n","        print(\"initialize data sets\")\n","        self.transform = transform\n","        rating_dict = {}\n","        self.image_list = list()\n","        self.score_list = list()\n","        years = [\"2021\"]\n","        for year in years:\n","            cnt = 0\n","            year_image_path = overall_image_path+\"iclr\"+year+\"/\" \n","\n","            with jsonlines.open(f\"{overall_image_path}/iclr{year}_metadata.jsonl\") as read_file:\n","                for line in read_file.iter():\n","                    rating_dict[line['forum']] = line['rating']\n","            input_paths = os.listdir(year_image_path)\n","            for one_file_image_path in tqdm(input_paths, desc=\"make data set\"):\n","                image_path = year_image_path + one_file_image_path + \"/\"\n","                before_add_size = len(self.image_list)\n","                self.image_list.extend(glob.glob(image_path + \"*.jpg\")) # glob: 폴더 내의 파일 찾아줌\n","                rating = rating_dict[one_file_image_path]\n","                self.score_list.extend([rating] * (len(self.image_list)-before_add_size))\n","                cnt += len(self.image_list)-before_add_size\n","            print(f\"{year}: {cnt}\")\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_list[idx]\n","        label = self.score_list[idx]\n","        img = Image.open(image_path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, label"],"metadata":{"id":"xw36Jb0yW9s-","executionInfo":{"status":"ok","timestamp":1651729612948,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# 새 섹션"],"metadata":{"id":"CNiQhauhcmXe"}},{"cell_type":"code","source":["image_path = \"drive/Shareddrives/소종-논문/\" \n","dataset_file_name = 'iclr2021_dataset.pt'"],"metadata":{"id":"cSfWpocEW9xt","executionInfo":{"status":"ok","timestamp":1651729615902,"user_tz":-540,"elapsed":487,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def make_save_data_set(image_path, dataset_file_name):\n","  print(\"start to make data set\")\n","  transform = transforms.Compose([\n","      transforms.Resize((224, 224)),\n","      transforms.ToTensor(),\n","  ])\n","\n","  dataset = PaperDataSet(image_path, transform=transform)\n","  print(f\"data set length: {dataset.__len__()}\")\n","\n","  torch.save(dataset, dataset_file_name)\n","  print(\"save data sets\")"],"metadata":{"id":"cgKR9l_DXcJ6","executionInfo":{"status":"ok","timestamp":1651729617472,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["make_save_data_set(image_path, dataset_file_name)\n","# data_set_usage_ex(dataset_file_name)"],"metadata":{"id":"Lt1_lnn8XcMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651729625265,"user_tz":-540,"elapsed":6589,"user":{"displayName":"정지우","userId":"03273628197078318973"}},"outputId":"0826eae0-b511-4de7-9ef0-b8610f1b71ab"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["start to make data set\n","initialize data sets\n"]},{"output_type":"stream","name":"stderr","text":["make data set: 100%|██████████| 2595/2595 [00:05<00:00, 447.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["2021: 33161\n","data set length: 33161\n","save data sets\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xEqyEffNZy6k","executionInfo":{"status":"ok","timestamp":1651729627116,"user_tz":-540,"elapsed":357,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.dataset import random_split\n","\n","dataset = torch.load(dataset_file_name)\n","\n","train_size = int(0.8 * len(dataset))\n","print(\"train size:\", train_size)\n","\n","test_size = len(dataset) - train_size\n","print(\"test size:\", test_size)\n","# validation \n","train_dataset, test_dataset = random_split(dataset, [train_size,test_size])\n","\n","test_size2=train_size-test_size\n","valid1_dataset, train1_dataset =  random_split(train_dataset, [test_size, test_size2])\n","test_size3=test_size2-test_size\n","valid2_dataset, remain2_dataset =  random_split( train1_dataset, [test_size, test_size3])\n","test_size4=test_size3-test_size\n","valid3_dataset, valid4_dataset =  random_split(remain2_dataset, [test_size, test_size4])\n","\n","train2_dataset= torch.utils.data.ConcatDataset([remain2_dataset, valid1_dataset])\n","doing_dataset= torch.utils.data.ConcatDataset([valid1_dataset, valid2_dataset])\n","train3_dataset= torch.utils.data.ConcatDataset([doing_dataset, valid4_dataset])\n","train4_dataset= torch.utils.data.ConcatDataset([doing_dataset, valid3_dataset])\n","\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n","\n","valid1_dataloader =  DataLoader(dataset=valid1_dataset, batch_size=128, shuffle=True)\n","valid2_dataloader =  DataLoader(dataset=valid2_dataset, batch_size=128, shuffle=True)\n","valid3_dataloader =  DataLoader(dataset=valid3_dataset, batch_size=128, shuffle=True)\n","valid4_dataloader =  DataLoader(dataset=valid4_dataset, batch_size=128, shuffle=True)\n","\n","train1_dataloader =  DataLoader(dataset=train1_dataset, batch_size=128, shuffle=True)\n","train2_dataloader =  DataLoader(dataset=train2_dataset, batch_size=128, shuffle=True)\n","train3_dataloader =  DataLoader(dataset=train3_dataset, batch_size=128, shuffle=True)\n","train4_dataloader =  DataLoader(dataset=train4_dataset, batch_size=128, shuffle=True)\n","\n","\n"],"metadata":{"id":"7WjyhkPoaxNj","executionInfo":{"status":"ok","timestamp":1651729628631,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지우","userId":"03273628197078318973"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea6b5ff3-33a2-4fa7-80d4-9d08aa988264"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["train size: 26528\n","test size: 6633\n"]}]},{"cell_type":"markdown","source":["02 training¶"],"metadata":{"id":"Bvjr_4qUNeYW"}},{"cell_type":"code","source":["from torchvision import models\n","import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 학습 환경 설정\n","\n","resnet50 = models.resnet50(pretrained=False).to(device) # true 옵션으로 사전 학습된 모델을 로드\n","\n","# transfer learning 사용 시 추가 \n","# if using_transfer_learning:\n","#   for param in resnet50.parameters():\n","#     param.requires_grad = False\n","\n","model = nn.Sequential(\n","    resnet50,\n","    nn.Flatten(),\n","    nn.Linear(1000, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, 32),\n","    nn.ReLU(),\n","    nn.Linear(32, 1)\n",").to(device)\n"],"metadata":{"id":"_UegSgy0NMSS","executionInfo":{"status":"ok","timestamp":1651729632246,"user_tz":-540,"elapsed":673,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch import optim\n","lr = 0.0001\n","num_epochs = 10\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","loss_function = nn.MSELoss().to(device)"],"metadata":{"id":"KaZHNA_HNk7b","executionInfo":{"status":"ok","timestamp":1651729633914,"user_tz":-540,"elapsed":339,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["params = {\n","    'num_epochs':num_epochs,\n","    'optimizer':optimizer,\n","    'loss_function':loss_function,\n","    'train_dataloader':train_dataloader,\n","    'train1_dataloader':train1_dataloader,\n","    'train2_dataloader':train2_dataloader,\n","    'train3_dataloader':train3_dataloader,\n","    'train4_dataloader':train4_dataloader,\n","    'valid1_dataloader':valid1_dataloader,\n","    'valid2_dataloader':valid2_dataloader,\n","    'valid3_dataloader':valid3_dataloader,\n","    'valid4_dataloader':valid4_dataloader,\n","    'test_dataloader': test_dataloader,\n","    'device':device\n","}\n"],"metadata":{"id":"9CHLTVscNqcv","executionInfo":{"status":"ok","timestamp":1651729635704,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"3HgavN8RgIgR","executionInfo":{"status":"ok","timestamp":1651729638521,"user_tz":-540,"elapsed":344,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","from numpy import vstack\n","\n","def validtrain1(model, params):\n","    total_start = time.time()\n","    loss_function=params[\"loss_function\"]\n","    train_dataloader=params[\"train_dataloader\"]\n","    train1_dataloader=params[\"train1_dataloader\"]\n","    \n","    valid1_dataloader=params[\"valid1_dataloader\"]\n","   \n","\n","    device=params[\"device\"]\n","\n","    print(\"start train\")\n","\n","    for epoch in range(0, 1):\n","      epoch_start = time.time()\n","      trained_number = 0\n","      for i, data in enumerate(train1_dataloader, 0):\n","        # train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","        # 이전 batch에서 계산된 가중치를 초기화\n","        optimizer.zero_grad() \n","        # forward + back propagation 연산\n","        outputs = model(inputs).squeeze()\n","        train_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32))\n","        train_loss.backward()\n","        optimizer.step()\n","        trained_number += labels.size(0)\n","        \n","        if i%50==0:\n","          print(f\"epoch {epoch+1} {trained_number/train_size*100}% train finish\")\n","        # break\n","      print(f\"epoch {epoch+1} train finish\") \n","\n","      \n","      # validation accuracy 계산\n","      total = 0\n","      correct = 0\n","      test_loss_list = list()\n","\n","      for i, data in enumerate(valid1_dataloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","\n","        # 결과값 연산\n","        outputs = model(inputs).squeeze()\n","\n","        total += labels.size(0)\n","        correct += (abs(outputs - labels)<0.5).sum().item() # 변경될 수 있음 \n","        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n","        test_loss_list.append(test_loss)\n","        if i%20==0:\n","          print(f\"epoch {epoch+1} {total/test_size*100}% validation finish\")\n","        # break\n","\n","      # 학습 결과 출력\n","      print('Epoch: %d/%d, Train loss: %.6f, validation loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n","\n","      epoch_elapsed_time = time.time() - epoch_start\n","      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n","      total_elapsed_time = time.time() - total_start\n","      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n","      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n","      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")  \n","\n","def validtrain2(model, params):\n","    total_start = time.time()\n","    loss_function=params[\"loss_function\"]\n","    train_dataloader=params[\"train_dataloader\"]\n","    \n","    train2_dataloader=params[\"train2_dataloader\"]\n","   \n","    valid2_dataloader=params[\"valid2_dataloader\"]\n","    \n","    device=params[\"device\"]\n","\n","    print(\"start train\")\n","\n","    for epoch in range(0, 1):\n","      epoch_start = time.time()\n","      trained_number = 0\n","      for i, data in enumerate(train2_dataloader, 0):\n","        # train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","        # 이전 batch에서 계산된 가중치를 초기화\n","        optimizer.zero_grad() \n","        # forward + back propagation 연산\n","        outputs = model(inputs).squeeze()\n","        train_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32))\n","        train_loss.backward()\n","        optimizer.step()\n","        trained_number += labels.size(0)\n","        \n","        if i%50==0:\n","          print(f\"epoch {epoch+1} {trained_number/train_size*100}% train finish\")\n","        # break\n","      print(f\"epoch {epoch+1} train finish\") \n","\n","      \n","      # validation accuracy 계산\n","      total = 0\n","      correct = 0\n","      test_loss_list = list()\n","\n","      for i, data in enumerate(valid2_dataloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","\n","        # 결과값 연산\n","        outputs = model(inputs).squeeze()\n","\n","        total += labels.size(0)\n","        correct += (abs(outputs - labels)<0.5).sum().item() # 변경될 수 있음 \n","        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n","        test_loss_list.append(test_loss)\n","        if i%20==0:\n","          print(f\"epoch {epoch+1} {total/test_size*100}% test finish\")\n","        # break\n","\n","      # 학습 결과 출력\n","      print('Epoch: %d/%d, Train loss: %.6f, validation loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n","\n","      epoch_elapsed_time = time.time() - epoch_start\n","      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n","      total_elapsed_time = time.time() - total_start\n","      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n","      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n","      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")  \n","\n","def validtrain3(model, params):\n","    total_start = time.time()\n","    loss_function=params[\"loss_function\"]\n","    train_dataloader=params[\"train_dataloader\"]\n","   \n","    train3_dataloader=params[\"train3_dataloader\"]\n","   \n","    valid3_dataloader=params[\"valid3_dataloader\"]\n","    \n","\n","    device=params[\"device\"]\n","\n","    print(\"start train\")\n","\n","    for epoch in range(0, 1):\n","      epoch_start = time.time()\n","      trained_number = 0\n","      for i, data in enumerate(train3_dataloader, 0):\n","        # train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","        # 이전 batch에서 계산된 가중치를 초기화\n","        optimizer.zero_grad() \n","        # forward + back propagation 연산\n","        outputs = model(inputs).squeeze()\n","        train_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32))\n","        train_loss.backward()\n","        optimizer.step()\n","        trained_number += labels.size(0)\n","        \n","        if i%50==0:\n","          print(f\"epoch {epoch+1} {trained_number/train_size*100}% train finish\")\n","        # break\n","      print(f\"epoch {epoch+1} train finish\") \n","\n","      \n","      # validation accuracy 계산\n","      total = 0\n","      correct = 0\n","      test_loss_list = list()\n","\n","      for i, data in enumerate(valid3_dataloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","\n","        # 결과값 연산\n","        outputs = model(inputs).squeeze()\n","\n","        total += labels.size(0)\n","        correct += (abs(outputs - labels)<0.5).sum().item() # 변경될 수 있음 \n","        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n","        test_loss_list.append(test_loss)\n","        if i%20==0:\n","          print(f\"epoch {epoch+1} {total/test_size*100}% test finish\")\n","        # break\n","\n","      # 학습 결과 출력\n","      print('Epoch: %d/%d, Train loss: %.6f, validation loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n","\n","      epoch_elapsed_time = time.time() - epoch_start\n","      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n","      total_elapsed_time = time.time() - total_start\n","      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n","      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n","      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")        \n","\n","def validtrain4(model, params):\n","    total_start = time.time()\n","    loss_function=params[\"loss_function\"]\n","    train_dataloader=params[\"train_dataloader\"]\n","\n","    train4_dataloader=params[\"train4_dataloader\"]\n","   \n","    valid4_dataloader=params[\"valid4_dataloader\"]\n","    \n","    device=params[\"device\"]\n","\n","    print(\"start train\")\n","\n","    for epoch in range(0, 1):\n","      epoch_start = time.time()\n","      trained_number = 0\n","      for i, data in enumerate(train4_dataloader, 0):\n","        # train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","        # 이전 batch에서 계산된 가중치를 초기화\n","        optimizer.zero_grad() \n","        # forward + back propagation 연산\n","        outputs = model(inputs).squeeze()\n","        train_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32))\n","        train_loss.backward()\n","        optimizer.step()\n","        trained_number += labels.size(0)\n","        \n","        if i%50==0:\n","          print(f\"epoch {epoch+1} {trained_number/train_size*100}% train finish\")\n","        # break\n","      print(f\"epoch {epoch+1} train finish\") \n","\n","      \n","      # validation accuracy 계산\n","      total = 0\n","      correct = 0\n","      test_loss_list = list()\n","\n","      for i, data in enumerate(valid4_dataloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","\n","        # 결과값 연산\n","        outputs = model(inputs).squeeze()\n","\n","        total += labels.size(0)\n","        correct += (abs(outputs - labels)<0.5).sum().item() # 변경될 수 있음 \n","        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n","        test_loss_list.append(test_loss)\n","        if i%20==0:\n","          print(f\"epoch {epoch+1} {total/test_size*100}% test finish\")\n","        # break\n","\n","      # 학습 결과 출력\n","      print('Epoch: %d/%d, Train loss: %.6f, validation loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n","\n","      epoch_elapsed_time = time.time() - epoch_start\n","      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n","      total_elapsed_time = time.time() - total_start\n","      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n","      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n","      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")  \n","\n","def train(model, params):\n","    total_start = time.time()\n","    loss_function=params[\"loss_function\"]\n","    train_dataloader=params[\"train_dataloader\"]\n","    \n","    test_dataloader=params[\"test_dataloader\"]\n","    device=params[\"device\"]\n","\n","   \n","    for epoch in range(0,1):\n","      \n","      # test accuracy 계산\n","      total = 0\n","      correct = 0\n","      test_loss_list = list()\n","\n","      for i, data in enumerate(test_dataloader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.type(torch.LongTensor) \n","        labels = labels.to(device)\n","        \n","\n","        # 결과값 연산\n","        outputs = model(inputs).squeeze()\n","\n","        total += labels.size(0)\n","        correct += (abs(outputs - labels)<0.5).sum().item() # 변경될 수 있음 \n","        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n","        test_loss_list.append(test_loss)\n","        if i%20==0:\n","          print(f\"epoch {epoch+1} {total/test_size*100}% test finish\")\n","        # break\n","\n","      # 학습 결과 출력\n","      print('Epoch: %d/%d,  Test loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs,  sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n","\n","      epoch_elapsed_time = time.time() - epoch_start\n","      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n","      total_elapsed_time = time.time() - total_start\n","      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n","      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n","      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")  "],"metadata":{"id":"W2ZnqIjCNqfQ","executionInfo":{"status":"ok","timestamp":1651729647438,"user_tz":-540,"elapsed":1200,"user":{"displayName":"정지우","userId":"03273628197078318973"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["validtrain1(model, params)\n","validtrain2(model, params)\n","validtrain3(model, params)\n","validtrain4(model, params)\n","\n","train(model, params)\n"],"metadata":{"id":"cmAGMDr2Nqhv","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"error","timestamp":1651729660978,"user_tz":-540,"elapsed":6779,"user":{"displayName":"정지우","userId":"03273628197078318973"}},"outputId":"0ae8e2cc-67ed-4bfb-92d4-22c8ade87dc7"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["start train\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-0db5daf7c189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvalidtrain3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidtrain4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-fa41a9bf0ce8>\u001b[0m in \u001b[0;36mvalidtrain1\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mtrained_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-87bf5de8b42d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2850\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2852\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"0zxw_uyDk52_"},"execution_count":null,"outputs":[]}]}