{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capstone-resnet50.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyTGlLXfNLii"
      },
      "outputs": [],
      "source": [
        "# torchvision 관련 라이브러리 import\n",
        "\n",
        "from torchvision import utils\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5uikNndWW_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. 데이터셋 로드"
      ],
      "metadata": {
        "id": "0GitHpa7XBmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "8x8E-xwbW_Qt",
        "outputId": "1b0ee80a-0cd9-413a-fcc5-4658ddf9c479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YmBD1wKovqoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "!pip install openreview-py\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "u3gRUgpEW9cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import tensorflow as tf\n",
        "import keras\n"
      ],
      "metadata": {
        "id": "0wgRUMyyW9je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "import openreview\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "from PIL import Image\n",
        "import jsonlines\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "XW5zuXp1W9nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "class PaperDataSet(Dataset):\n",
        "    def __init__(self, overall_image_path, transform=None):\n",
        "        print(\"initialize data sets\")\n",
        "        self.transform = transform\n",
        "        rating_dict = {}\n",
        "        self.image_list = list()\n",
        "        self.score_list = list()\n",
        "        years = [\"2021\"]\n",
        "        for year in years:\n",
        "            cnt = 0\n",
        "            year_image_path = overall_image_path+\"iclr\"+year+\"/\"\n",
        "\n",
        "            with jsonlines.open(f\"{overall_image_path}/iclr{year}_metadata.jsonl\") as read_file:\n",
        "                for line in read_file.iter():\n",
        "                    rating_dict[line['forum']] = line['rating']\n",
        "            input_paths = os.listdir(year_image_path)\n",
        "            for one_file_image_path in tqdm(input_paths, desc=\"make data set\"):\n",
        "                image_path = year_image_path + one_file_image_path + \"/\"\n",
        "                before_add_size = len(self.image_list)\n",
        "                self.image_list.extend(glob.glob(image_path + \"*.jpg\")) # glob: 폴더 내의 파일 찾아줌\n",
        "                rating = rating_dict[one_file_image_path]\n",
        "                self.score_list.extend([rating] * (len(self.image_list)-before_add_size))\n",
        "                cnt += len(self.image_list)-before_add_size\n",
        "            print(f\"{year}: {cnt}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_list[idx]\n",
        "        label = self.score_list[idx]\n",
        "        img = Image.open(image_path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "xw36Jb0yW9s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"drive/Shareddrives/소종-논문/\" \n",
        "dataset_file_name = 'iclr2021_dataset.pt'"
      ],
      "metadata": {
        "id": "cSfWpocEW9xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_save_data_set(image_path, dataset_file_name):\n",
        "  print(\"start to make data set\")\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "  ])\n",
        "\n",
        "  dataset = PaperDataSet(image_path, transform=transform)\n",
        "  print(f\"data set length: {dataset.__len__()}\")\n",
        "\n",
        "  torch.save(dataset, dataset_file_name)\n",
        "  print(\"save data sets\")"
      ],
      "metadata": {
        "id": "cgKR9l_DXcJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_save_data_set(image_path, dataset_file_name)\n",
        "# data_set_usage_ex(dataset_file_name)"
      ],
      "metadata": {
        "id": "Lt1_lnn8XcMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xEqyEffNZy6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "dataset = torch.load(dataset_file_name)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "print(\"train size:\", train_size)\n",
        "\n",
        "test_size = len(dataset) - train_size\n",
        "print(\"test size:\", test_size)\n",
        "# validation \n",
        "train_dataset, test_dataset = random_split(dataset, [train_size,test_size])\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "7WjyhkPoaxNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "02 training¶"
      ],
      "metadata": {
        "id": "Bvjr_4qUNeYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 학습 환경 설정\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=False).to(device) # true 옵션으로 사전 학습된 모델을 로드\n",
        "\n",
        "# transfer learning 사용 시 추가 \n",
        "# if using_transfer_learning:\n",
        "#   for param in resnet50.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "model = nn.Sequential(\n",
        "    resnet50,\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1000, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1)\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "_UegSgy0NMSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "lr = 0.0001\n",
        "num_epochs = 10\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "loss_function = nn.MSELoss().to(device)"
      ],
      "metadata": {
        "id": "KaZHNA_HNk7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'num_epochs':num_epochs,\n",
        "    'optimizer':optimizer,\n",
        "    'loss_function':loss_function,\n",
        "    'train_dataloader':train_dataloader,\n",
        "    'test_dataloader': test_dataloader,\n",
        "    'device':device\n",
        "}"
      ],
      "metadata": {
        "id": "9CHLTVscNqcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "3HgavN8RgIgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "from numpy import vstack\n",
        "\n",
        "def train(model, params):\n",
        "    total_start = time.time()\n",
        "    loss_function=params[\"loss_function\"]\n",
        "    train_dataloader=params[\"train_dataloader\"]\n",
        "    test_dataloader=params[\"test_dataloader\"]\n",
        "    device=params[\"device\"]\n",
        "\n",
        "    print(\"start train\")\n",
        "    print(\"train size:\", train_size)\n",
        "    print(\"test size:\", test_size)\n",
        "    for epoch in range(0, num_epochs):\n",
        "      epoch_start = time.time()\n",
        "      trained_number = 0\n",
        "      for i, data in enumerate(train_dataloader, 0):\n",
        "        # train dataloader 로 불러온 데이터에서 이미지와 라벨을 분리\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.type(torch.LongTensor) \n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # 이전 batch에서 계산된 가중치를 초기화\n",
        "        optimizer.zero_grad() \n",
        "        # forward + back propagation 연산\n",
        "        outputs = model(inputs).squeeze()\n",
        "        train_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32))\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        trained_number += labels.size(0)\n",
        "        \n",
        "        if i%50==0:\n",
        "          print(f\"epoch {epoch+1} {trained_number/train_size*100}% train finish\")\n",
        "        # break\n",
        "      print(f\"epoch {epoch+1} train finish\") \n",
        "      # test accuracy 계산\n",
        "      total = 0\n",
        "      correct = 0\n",
        "      test_loss_list = list()\n",
        "\n",
        "      for i, data in enumerate(test_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.type(torch.LongTensor) \n",
        "        labels = labels.to(device)\n",
        "        \n",
        "\n",
        "        # 결과값 연산\n",
        "        outputs = model(inputs).squeeze()\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (abs(outputs - labels)<0.5).sum().item()\n",
        "        test_loss = loss_function(outputs.to(torch.float32), labels.to(torch.float32)).item()\n",
        "        test_loss_list.append(test_loss)\n",
        "        if i%20==0:\n",
        "          print(f\"epoch {epoch+1} {total/test_size*100}% test finish\")\n",
        "        break\n",
        "\n",
        "      # 학습 결과 출력\n",
        "      print('Epoch: %d/%d, Train loss: %.6f, Test loss: %.6f, Accuracy: %.2f' %(epoch+1, num_epochs, train_loss.item(), sum(test_loss_list)/len(test_loss_list), 100*correct/total))\n",
        "\n",
        "      epoch_elapsed_time = time.time() - epoch_start\n",
        "      epoch_elapsed_time_list = str(datetime.timedelta(seconds=epoch_elapsed_time)).split(\".\")\n",
        "      total_elapsed_time = time.time() - total_start\n",
        "      total_elapsed_time_list = str(datetime.timedelta(seconds=total_elapsed_time)).split(\".\")\n",
        "      print(f\"Epoch {epoch+1} Elapsed time is {epoch_elapsed_time_list[0]}\")  \n",
        "      print(f\"Total Elapsed time is {total_elapsed_time_list[0]}\")  "
      ],
      "metadata": {
        "id": "W2ZnqIjCNqfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, params)\n"
      ],
      "metadata": {
        "id": "cmAGMDr2Nqhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0zxw_uyDk52_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}